{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Section 3 on MNIST \n",
    "\n",
    "Large NN with 2 hidden layers and 1200 hidden units EACH (60k training cases from MNIST set?)\n",
    "Strongly regularized with dropout and weight constraints (listed somewhere)\n",
    "MNIST dataset was modified using jiterring in 2 pixels in a random direction\n",
    "\n",
    "SMALLER student network had 2 hidden layers of 800 hidden units and NO regularization\n",
    "Regularized by adding task of matching soft targets from large network at temperature 20 (fusing?)\n",
    "T >= 8 at 300 or more units\n",
    "\n",
    "So main idea is that you create a distillation loss which compares softmax soft output to teacher soft output\n",
    "- e.g. Logit matching... (nope this is a special case?)\n",
    "- How to get teacher soft output instead of the class? guess you just don't modify it?\n",
    "\n",
    "\n",
    "Steps: (softmax vs logit = prob vs logodds)\n",
    "1. Train teacher on MNIST using low temperature\n",
    "2. Train student on MNIST using distillation loss with high temperature on teacher softmax output...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main idea of paper is to train smaller model using SOFT output distribution from larger model\n",
    "Softmax of the student will be higher than the parent to smooth it?\n",
    "\n",
    "T in teacher usually 1\n",
    "T in student is higher (softer probability over classes = more volatile though?) T=20 (hyperparameter)\n",
    "\n",
    "Implement 2 stage training (just nest normal training loop into the student train function)\n",
    "\n",
    "Set parent to eval then student to train mode\n",
    "Transfer set? How to build this?\n",
    "\"\"\"\n",
    "def distillation_loss(student_logits, teacher_logits, labels, T=2.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Compute the knowledge distillation loss.\n",
    "    \n",
    "    Args:\n",
    "        student_logits: Logits from the student model\n",
    "        teacher_logits: Logits from the teacher model\n",
    "        labels: True labels\n",
    "        T: Temperature for softening probability distributions\n",
    "        alpha: Weight for the distillation loss vs. standard cross-entropy loss\n",
    "    \n",
    "    Returns:\n",
    "        Combined loss\n",
    "    \"\"\"\n",
    "    # Softmax with temperature for soft targets\n",
    "    soft_targets = F.softmax(teacher_logits / T, dim=1)\n",
    "    soft_prob = F.log_softmax(student_logits / T, dim=1)\n",
    "    \n",
    "    # Calculate the distillation loss (soft targets)\n",
    "    # The T^2 term is to scale the gradients appropriately\n",
    "    distillation = F.kl_div(soft_prob, soft_targets, reduction='batchmean') * (T * T)\n",
    "    \n",
    "    # Calculate the standard cross-entropy loss (hard targets)\n",
    "    standard_loss = F.cross_entropy(student_logits, labels)\n",
    "    \n",
    "    # Return the weighted sum\n",
    "    return alpha * distillation + (1 - alpha) * standard_loss\n",
    "\n",
    "# Training function for the student model\n",
    "def train_student(teacher_model, student_model, train_loader, optimizer, device, T=2.0, alpha=0.5):\n",
    "    teacher_model.eval()  # Teacher model in evaluation mode\n",
    "    student_model.train()  # Student model in training mode\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with the teacher model (no gradient calculation needed)\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher_model(data)\n",
    "        \n",
    "        # Forward pass with the student model\n",
    "        student_logits = student_model(data)\n",
    "        \n",
    "        # Calculate the combined loss\n",
    "        loss = distillation_loss(student_logits, teacher_logits, target, T, alpha)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load MNIST dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.MNIST('./data', train=False, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1000)\n",
    "    \n",
    "    # Initialize teacher and student models\n",
    "    teacher_model = SimpleCNN(num_hidden=1200).to(device)  # Larger teacher model\n",
    "    student_model = SimpleCNN(num_hidden=800).to(device)   # Smaller student model\n",
    "    \n",
    "    # We assume the teacher model has already been trained\n",
    "    # teacher_model.load_state_dict(torch.load('teacher_model.pth'))\n",
    "    \n",
    "    # Here we would train the teacher model first\n",
    "    # For brevity, assume teacher model is already trained\n",
    "    \n",
    "    # Create an optimizer for the student model\n",
    "    optimizer = optim.SGD(student_model.parameters(), lr=0.01, momentum=0.9)\n",
    "    \n",
    "    # Train the student model with knowledge distillation\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = train_student(teacher_model, student_model, train_loader, optimizer, device, T=20.0, alpha=0.5)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    # Save the distilled student model\n",
    "    torch.save(student_model.state_dict(), 'distilled_student_model.pth')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
