{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "project_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(project_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training batch inputs: torch.Size([32, 1, 28, 28])\n",
      "Shape of training batch labels: torch.Size([32])\n",
      "Shape of testing batch inputs: torch.Size([32, 1, 28, 28])\n",
      "Shape of testing batch labels: torch.Size([32])\n",
      "MNIST dataset loaded into PyTorch DataLoaders.\n"
     ]
    }
   ],
   "source": [
    "from data.mnist import get_mnist_pipeline\n",
    "\n",
    "train_loader, val_loader, test_loader = get_mnist_pipeline(batch_size=32)\n",
    "for i, data in enumerate(train_loader, 0):\n",
    "    inputs, labels = data\n",
    "    if i == 0:\n",
    "        print(\"Shape of training batch inputs:\", inputs.shape)\n",
    "        print(\"Shape of training batch labels:\", labels.shape)\n",
    "        break\n",
    "for i, data in enumerate(val_loader, 0):\n",
    "    inputs, labels = data\n",
    "    if i == 0:\n",
    "        print(\"Shape of testing batch inputs:\", inputs.shape)\n",
    "        print(\"Shape of testing batch labels:\", labels.shape)\n",
    "        break\n",
    "print(\"MNIST dataset loaded into PyTorch DataLoaders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher model output shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from models.baseline import BasicMLP\n",
    "from utils.summary import get_model_flops\n",
    "\n",
    "teacher = BasicMLP(hidden_size=1200)\n",
    "student = BasicMLP(hidden_size=400)\n",
    "\n",
    "sample = torch.randn(1, 1, 28, 28)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = teacher(sample)\n",
    "    print(\"Teacher model output shape:\", pred.shape)\n",
    "    # print(\"Teacher model FLOPs:\", get_model_flops(teacher, sample.shape[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_model(train_loader: DataLoader, model: nn.Module, criterion: nn.Module, optimizer: nn.Module,\n",
    "                scheduler: nn.Module=None, device: str='cpu') -> list:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to train.\n",
    "        train_loader: DataLoader for the training data.\n",
    "        optimizer: Optimizer for updating model parameters.\n",
    "        criterion: Loss function.\n",
    "        device: Device to run the training on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        list: Collection of train losses.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    for inputs, targets in tqdm.tqdm(train_loader, desc='training...', file=sys.stdout):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        preds = model(inputs)\n",
    "        loss = criterion(preds, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        if scheduler and isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(loss.item())\n",
    "        elif scheduler:\n",
    "            scheduler.step()\n",
    "    return epoch_losses\n",
    "\n",
    "\n",
    "def evaluate_model(val_loader: DataLoader, model: nn.Module, criterion: nn.Module, device: str='cpu') -> list:\n",
    "    \"\"\"\n",
    "    Evaluate the model on validation data.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to evaluate.\n",
    "        val_loader: DataLoader for the validation data.\n",
    "        criterion: Loss function.\n",
    "        device: Device to run the evaluation on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        list: Collection of metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_metrics = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm.tqdm(val_loader, desc='evaluating...', file=sys.stdout):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            preds = model(inputs)\n",
    "            loss = criterion(preds, targets)\n",
    "            epoch_metrics.append(loss.item())\n",
    "    return epoch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "training...: 100%|██████████| 1500/1500 [00:13<00:00, 112.85it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 160.89it/s]\n",
      "Epoch 1: New best val loss: 0.6518, saving model...\n",
      "training...: 100%|██████████| 1500/1500 [00:11<00:00, 129.90it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 167.68it/s]\n",
      "Epoch 2: New best val loss: 0.6467, saving model...\n",
      "training...: 100%|██████████| 1500/1500 [00:13<00:00, 110.51it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 166.37it/s]\n",
      "training...: 100%|██████████| 1500/1500 [00:11<00:00, 126.31it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 164.37it/s]\n",
      "training...: 100%|██████████| 1500/1500 [00:11<00:00, 127.61it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 166.43it/s]\n",
      "training...: 100%|██████████| 1500/1500 [00:13<00:00, 111.70it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 164.40it/s]\n",
      "training...: 100%|██████████| 1500/1500 [00:11<00:00, 129.30it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 179.37it/s]\n",
      "Epoch 7: New best val loss: 0.6434, saving model...\n",
      "training...: 100%|██████████| 1500/1500 [00:13<00:00, 112.75it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 165.52it/s]\n",
      "training...: 100%|██████████| 1500/1500 [00:11<00:00, 130.34it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 172.03it/s]\n",
      "training...: 100%|██████████| 1500/1500 [00:13<00:00, 113.79it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 169.91it/s]\n",
      "training...: 100%|██████████| 1500/1500 [00:11<00:00, 136.04it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 181.18it/s]\n",
      "training...:  37%|███▋      | 551/1500 [00:04<00:07, 121.45it/s]"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "print(device)\n",
    "\n",
    "teacher = teacher.to(device)\n",
    "criterion = nn.CrossEntropyLoss()   # temperatione?\n",
    "optimizer = optim.Adam(teacher.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "metrics = {\"train_loss\": [], \"val_loss\": []}\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "counter = 0\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_model(train_loader, teacher, criterion, optimizer, scheduler, device)\n",
    "    val_loss = evaluate_model(val_loader, teacher, criterion, device)\n",
    "    metrics['train_loss'].append(np.mean(train_loss))\n",
    "    metrics['val_loss'].append(np.mean(val_loss))\n",
    "    if metrics['val_loss'][-1] < best_val_loss:\n",
    "        best_val_loss = metrics['val_loss'][-1]\n",
    "        counter = 0\n",
    "        print(f\"Epoch {epoch+1}: New best val loss: {best_val_loss:.4f}, saving model...\")\n",
    "        torch.save(teacher.state_dict(), '../models/pretrained/teacher.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "    if counter >= patience:\n",
    "        print(f\"Epoch {epoch+1}: Early stop triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.losses import DistillationLoss\n",
    "\n",
    "\n",
    "def distill_model(train_loader: DataLoader, student: nn.Module, teacher: nn.Module, \n",
    "                  criterion: nn.Module, optimizer: nn.Module,\n",
    "                  scheduler: nn.Module=None, device: str='cpu') -> list:\n",
    "    \"\"\"\n",
    "    Train the student model for one epoch using knowledge distillation.\n",
    "\n",
    "    Args:\n",
    "        train_loader: DataLoader for the training data.\n",
    "        student_model: The PyTorch student model to train.\n",
    "        teacher_model: The PyTorch teacher model (should be in eval mode).\n",
    "        optimizer: Optimizer for updating student model parameters.\n",
    "        criterion: Distillation loss function (e.g., DistillationLoss).\n",
    "        device: Device to run the training on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        list: Collection of train losses.\n",
    "    \"\"\"\n",
    "    student.train()\n",
    "    teacher.eval()  # Ensure teacher model is in evaluation mode\n",
    "    epoch_losses = []\n",
    "    for inputs, targets in tqdm.tqdm(train_loader, desc='distilling...', file=sys.stdout):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # preds\n",
    "        student_preds = student(inputs)\n",
    "        with torch.no_grad():\n",
    "            teacher_preds = teacher(inputs)\n",
    "        # distillation loss\n",
    "        loss = criterion(student_preds, teacher_preds, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        if scheduler and isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(loss.item())\n",
    "        elif scheduler:\n",
    "            scheduler.step()\n",
    "    return epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher.load_state_dict(torch.load('../models/pretrained/teacher.pth', map_location=device))\n",
    "student = student.to(device)\n",
    "criterion = DistillationLoss(T=20)\n",
    "optimizer = optim.Adam(teacher.parameters(), lr=0.001)  # remove regularization\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "metrics = {\"train_loss\": [], \"val_loss\": []}\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "counter = 0\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_model(train_loader, teacher, criterion, optimizer, scheduler, device)\n",
    "    val_loss = evaluate_model(val_loader, student, criterion, device)\n",
    "    metrics['train_loss'].append(np.mean(train_loss))\n",
    "    metrics['val_loss'].append(np.mean(val_loss))\n",
    "    if metrics['val_loss'][-1] < best_val_loss:\n",
    "        best_val_loss = metrics['val_loss'][-1]\n",
    "        counter = 0\n",
    "        print(f\"Epoch {epoch+1}: New best val loss: {best_val_loss:.4f}, saving model...\")\n",
    "        torch.save(teacher.state_dict(), '../models/pretrained/student.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "    if counter >= patience:\n",
    "        print(f\"Epoch {epoch+1}: Early stop triggered.\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock_ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
