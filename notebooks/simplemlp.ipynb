{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "project_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training batch inputs: torch.Size([32, 1, 28, 28])\n",
      "Shape of training batch labels: torch.Size([32])\n",
      "Shape of validation batch inputs: torch.Size([32, 1, 28, 28])\n",
      "Shape of validation batch labels: torch.Size([32])\n",
      "Shape of testing batch inputs: torch.Size([32, 1, 28, 28])\n",
      "Shape of testing batch labels: torch.Size([32])\n",
      "MNIST dataset loaded into PyTorch DataLoaders.\n"
     ]
    }
   ],
   "source": [
    "from data.mnist import get_mnist_pipeline\n",
    "\n",
    "train_loader, val_loader, test_loader = get_mnist_pipeline(batch_size=32)\n",
    "for i, data in enumerate(train_loader, 0):\n",
    "    inputs, labels = data\n",
    "    if i == 0:\n",
    "        print(\"Shape of training batch inputs:\", inputs.shape)\n",
    "        print(\"Shape of training batch labels:\", labels.shape)\n",
    "        break\n",
    "for i, data in enumerate(val_loader, 0):\n",
    "    inputs, labels = data\n",
    "    if i == 0:\n",
    "        print(\"Shape of validation batch inputs:\", inputs.shape)\n",
    "        print(\"Shape of validation batch labels:\", labels.shape)\n",
    "        break\n",
    "for i, data in enumerate(test_loader, 0):\n",
    "    inputs, labels = data\n",
    "    if i == 0:\n",
    "        print(\"Shape of testing batch inputs:\", inputs.shape)\n",
    "        print(\"Shape of testing batch labels:\", labels.shape)\n",
    "        break\n",
    "print(\"MNIST dataset loaded into PyTorch DataLoaders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher model stats:\n",
      "flops: 2392800\n",
      "params: 2395210\n",
      "Student model stats:\n",
      "flops: 477600\n",
      "params: 478410\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import models.baseline as mlp\n",
    "from utils.summary import get_model_stats\n",
    "\n",
    "teacher = mlp.mnist1200().to(device)\n",
    "student = mlp.mnist400().to(device)\n",
    "smaller = mlp.mnist400().to(device)\n",
    "\n",
    "sample = torch.randn(1, 1, 28, 28).to(device)\n",
    "with torch.no_grad():\n",
    "    print(\"Teacher model stats:\")\n",
    "    pred = teacher(sample)\n",
    "    for name, param in get_model_stats(teacher, sample.shape).items():\n",
    "        print(f\"{name}: {param}\")\n",
    "    print(\"Student model stats:\")\n",
    "    pred = smaller(sample)\n",
    "    for name, param in get_model_stats(student, sample.shape).items():\n",
    "        print(f\"{name}: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "# container for things to pass to train_val and eval\n",
    "\n",
    "@dataclass\n",
    "class ScriptArgs:\n",
    "    model: str\n",
    "    criterion: str\n",
    "    optimizer: str   # lr included in optimizer\n",
    "    scheduler: str\n",
    "    device: str\n",
    "    epochs: int\n",
    "    lr: float\n",
    "    metrics: dict\n",
    "    path: str\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # instantiate these\n",
    "        self.criterion = None\n",
    "        self.optimizer = getattr(optim, self.optimizer)  # create the optimizer\n",
    "        self.scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_model(train_loader: DataLoader, model: nn.Module, criterion: nn.Module, optimizer: nn.Module,\n",
    "                scheduler: nn.Module=None, device: str='cpu') -> list:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to train.\n",
    "        train_loader: DataLoader for the training data.\n",
    "        optimizer: Optimizer for updating model parameters.\n",
    "        criterion: Loss function.\n",
    "        device: Device to run the training on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        list: Collection of train losses.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    for inputs, targets in tqdm.tqdm(train_loader, desc='training...', file=sys.stdout):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        preds = model(inputs)\n",
    "        loss = criterion(preds, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        if scheduler and isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(loss.item())\n",
    "        elif scheduler:\n",
    "            scheduler.step()\n",
    "    return epoch_losses\n",
    "\n",
    "\n",
    "def evaluate_model(val_loader: DataLoader, model: nn.Module, criterion: nn.Module, device: str='cpu') -> list:\n",
    "    \"\"\"\n",
    "    Evaluate the model on validation data.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to evaluate.\n",
    "        val_loader: DataLoader for the validation data.\n",
    "        criterion: Loss function.\n",
    "        device: Device to run the evaluation on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        list: Collection of metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_metrics = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm.tqdm(val_loader, desc='evaluating...', file=sys.stdout):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            preds = model(inputs)\n",
    "            loss = criterion(preds, targets)\n",
    "            epoch_metrics.append(loss.item())\n",
    "    return epoch_metrics\n",
    "\n",
    "def train_val(model, train_loader, val_loader, criterion, optimizer, scheduler, device, path, metrics={}):\n",
    "    \"\"\"\n",
    "    Trains and Validates Model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        best_val_loss = torch.load(path)['val_loss']\n",
    "    except FileNotFoundError:\n",
    "        best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    epochs = 200\n",
    "    \n",
    "    metrics.update({'train_loss': [], 'val_loss': []})\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # dump config + add device\n",
    "        train_loss = train_model(train_loader, model, criterion, optimizer, scheduler, device)\n",
    "        val_loss = evaluate_model(val_loader, model, criterion, device)\n",
    "        # pop metrics from config\n",
    "        metrics['train_loss'].append(np.mean(train_loss))\n",
    "        metrics['val_loss'].append(np.mean(val_loss))\n",
    "        if metrics['val_loss'][-1] < best_val_loss:\n",
    "            best_val_loss = metrics['val_loss'][-1]\n",
    "            counter = 0\n",
    "            print(f\"Epoch {epoch+1}: New best val loss: {best_val_loss:.4f}, saving model...\")\n",
    "            state = {\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'val_loss': best_val_loss\n",
    "            }\n",
    "            torch.save(state, path)\n",
    "        else:\n",
    "            counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Epoch {epoch+1}: Early stop triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "training...: 100%|██████████| 1500/1500 [00:13<00:00, 112.85it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 160.89it/s]\n",
      "Epoch 1: New best val loss: 0.6518, saving model...\n",
      "training...: 100%|██████████| 1500/1500 [00:11<00:00, 129.90it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 167.68it/s]\n",
      "Epoch 2: New best val loss: 0.6467, saving model...\n",
      "training...: 100%|██████████| 1500/1500 [00:13<00:00, 110.51it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 166.37it/s]\n",
      "training...: 100%|██████████| 1500/1500 [00:11<00:00, 126.31it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 164.37it/s]\n",
      "training...: 100%|██████████| 1500/1500 [00:11<00:00, 127.61it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 166.43it/s]\n",
      "training...: 100%|██████████| 1500/1500 [00:13<00:00, 111.70it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 164.40it/s]\n",
      "training...: 100%|██████████| 1500/1500 [00:11<00:00, 129.30it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 179.37it/s]\n",
      "Epoch 7: New best val loss: 0.6434, saving model...\n",
      "training...: 100%|██████████| 1500/1500 [00:13<00:00, 112.75it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 165.52it/s]\n",
      "training...: 100%|██████████| 1500/1500 [00:11<00:00, 130.34it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 172.03it/s]\n",
      "training...: 100%|██████████| 1500/1500 [00:13<00:00, 113.79it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 169.91it/s]\n",
      "training...: 100%|██████████| 1500/1500 [00:11<00:00, 136.04it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 181.18it/s]\n",
      "training...: 100%|██████████| 1500/1500 [00:11<00:00, 129.04it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 169.58it/s]\n",
      "Epoch 12: New best val loss: 0.6408, saving model...\n",
      "training...: 100%|██████████| 1500/1500 [00:13<00:00, 115.28it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 163.29it/s]\n",
      "training...: 100%|██████████| 1500/1500 [00:11<00:00, 132.10it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 175.69it/s]\n",
      "training...: 100%|██████████| 1500/1500 [00:12<00:00, 115.55it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 166.04it/s]\n",
      "training...: 100%|██████████| 1500/1500 [00:11<00:00, 127.08it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 163.76it/s]\n",
      "training...: 100%|██████████| 1500/1500 [00:13<00:00, 110.07it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 149.27it/s]\n",
      "training...: 100%|██████████| 1500/1500 [00:12<00:00, 121.29it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 163.95it/s]\n",
      "training...: 100%|██████████| 1500/1500 [00:11<00:00, 129.46it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:04<00:00, 91.86it/s] \n",
      "training...: 100%|██████████| 1500/1500 [00:11<00:00, 128.05it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 162.13it/s]\n",
      "training...: 100%|██████████| 1500/1500 [00:11<00:00, 129.86it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 167.86it/s]\n",
      "training...: 100%|██████████| 1500/1500 [00:12<00:00, 115.59it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 168.03it/s]\n",
      "Epoch 22: Early stop triggered.\n"
     ]
    }
   ],
   "source": [
    "teacher = teacher.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(teacher.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "metrics = {\"train_loss\": [], \"val_loss\": []}\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "counter = 0\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_model(train_loader, teacher, criterion, optimizer, scheduler, device)\n",
    "    val_loss = evaluate_model(val_loader, teacher, criterion, device)\n",
    "    metrics['train_loss'].append(np.mean(train_loss))\n",
    "    metrics['val_loss'].append(np.mean(val_loss))\n",
    "    if metrics['val_loss'][-1] < best_val_loss:\n",
    "        best_val_loss = metrics['val_loss'][-1]\n",
    "        counter = 0\n",
    "        print(f\"Epoch {epoch+1}: New best val loss: {best_val_loss:.4f}, saving model...\")\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': teacher.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'val_loss': best_val_loss\n",
    "        }\n",
    "        torch.save(state, '../models/weights/teacher.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "    if counter >= patience:\n",
    "        print(f\"Epoch {epoch+1}: Early stop triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(smaller.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "metrics = {\"train_loss\": [], \"val_loss\": []}\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "counter = 0\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_model(train_loader, smaller, criterion, optimizer, scheduler, device)\n",
    "    val_loss = evaluate_model(val_loader, smaller, criterion, device)\n",
    "    metrics['train_loss'].append(np.mean(train_loss))\n",
    "    metrics['val_loss'].append(np.mean(val_loss))\n",
    "    if metrics['val_loss'][-1] < best_val_loss:\n",
    "        best_val_loss = metrics['val_loss'][-1]\n",
    "        counter = 0\n",
    "        print(f\"Epoch {epoch+1}: New best val loss: {best_val_loss:.4f}, saving model...\")\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': smaller.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'val_loss': best_val_loss\n",
    "        }\n",
    "        torch.save(state, '../models/weights/smaller.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "    if counter >= patience:\n",
    "        print(f\"Epoch {epoch+1}: Early stop triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distill Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.losses import DistillationLoss\n",
    "\n",
    "\n",
    "def distill_model(train_loader: DataLoader, student: nn.Module, teacher: nn.Module, \n",
    "                  criterion: nn.Module, optimizer: nn.Module,\n",
    "                  scheduler: nn.Module=None, device: str='cpu') -> list:\n",
    "    \"\"\"\n",
    "    Train the student model for one epoch using knowledge distillation.\n",
    "\n",
    "    Args:\n",
    "        train_loader: DataLoader for the training data.\n",
    "        student_model: The PyTorch student model to train.\n",
    "        teacher_model: The PyTorch teacher model (should be in eval mode).\n",
    "        optimizer: Optimizer for updating student model parameters.\n",
    "        criterion: Distillation loss function (e.g., DistillationLoss).\n",
    "        device: Device to run the training on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        list: Collection of train losses.\n",
    "    \"\"\"\n",
    "    student.train()\n",
    "    teacher.eval()  # Ensure teacher model is in evaluation mode\n",
    "    epoch_losses = []\n",
    "    for inputs, targets in tqdm.tqdm(train_loader, desc='distilling...', file=sys.stdout):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # preds\n",
    "        student_preds = student(inputs)\n",
    "        with torch.no_grad():\n",
    "            teacher_preds = teacher(inputs)\n",
    "        # distillation loss\n",
    "        loss = criterion(student_preds, teacher_preds, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        if scheduler and isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(loss.item())\n",
    "        elif scheduler:\n",
    "            scheduler.step()\n",
    "    return epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_245653/3874728733.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  teacher.load_state_dict(torch.load('../models/pretrained/teacher.pth', map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilling...: 100%|██████████| 1500/1500 [00:11<00:00, 134.87it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 162.68it/s]\n",
      "Epoch 1: New best val loss: 2.3134, saving model...\n",
      "distilling...: 100%|██████████| 1500/1500 [00:13<00:00, 112.94it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 158.44it/s]\n",
      "Epoch 2: New best val loss: 2.3129, saving model...\n",
      "distilling...: 100%|██████████| 1500/1500 [00:11<00:00, 128.90it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 156.66it/s]\n",
      "distilling...: 100%|██████████| 1500/1500 [00:14<00:00, 105.95it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 153.57it/s]\n",
      "distilling...: 100%|██████████| 1500/1500 [00:12<00:00, 124.79it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 165.77it/s]\n",
      "distilling...: 100%|██████████| 1500/1500 [00:12<00:00, 119.59it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:04<00:00, 88.98it/s] \n",
      "distilling...: 100%|██████████| 1500/1500 [00:12<00:00, 117.42it/s]\n",
      "evaluating...: 100%|██████████| 375/375 [00:02<00:00, 155.93it/s]\n",
      "Epoch 7: Early stop triggered.\n"
     ]
    }
   ],
   "source": [
    "teacher.load_state_dict(torch.load('../models/weights/teacher.pth', map_location=device)['state_dict'])\n",
    "student = student.to(device)\n",
    "criterion = DistillationLoss(T=20)\n",
    "crossentropy = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(teacher.parameters(), lr=0.001)  # remove regularization\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "metrics = {\"train_loss\": [], \"val_loss\": []}\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "counter = 0\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = distill_model(train_loader, student, teacher, criterion, optimizer, scheduler, device)\n",
    "    val_loss = evaluate_model(val_loader, student, crossentropy, device)\n",
    "    metrics['train_loss'].append(np.mean(train_loss))\n",
    "    metrics['val_loss'].append(np.mean(val_loss))\n",
    "    if metrics['val_loss'][-1] < best_val_loss:\n",
    "        best_val_loss = metrics['val_loss'][-1]\n",
    "        counter = 0\n",
    "        print(f\"Epoch {epoch+1}: New best val loss: {best_val_loss:.4f}, saving model...\")\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': student.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'val_loss': best_val_loss\n",
    "        }\n",
    "        torch.save(state, '../models/weights/student.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "    if counter >= patience:\n",
    "        print(f\"Epoch {epoch+1}: Early stop triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare: Teacher, Smaller, Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, test_loader, criterion, device, metrics={}):\n",
    "    # abstract to model rather than teacher/small    \n",
    "    metrics.update({'test_loss': []})\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # dump config + add device\n",
    "        test_loss = evaluate_model(test_loader, model, criterion, device)\n",
    "        # pop metrics from config\n",
    "        metrics['test_loss'].append(np.mean(test_loss))\n",
    "        \n",
    "def plugins():\n",
    "    \"\"\"Some visualization bs\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_state = torch.load('../models/weights/teacher.pth', map_location=device)\n",
    "student_state = torch.load('../models/weights/student.pth', map_location=device)\n",
    "smaller_state = torch.load('../models/weights/smaller.pth', map_location=device)\n",
    "teacher.load_state_dict(teacher_state['state_dict'])\n",
    "student.load_state_dict(student_state['state_dict'])\n",
    "smaller.load_state_dict(smaller_state['state_dict'])\n",
    "\n",
    "# look at the best val_losses\n",
    "\n",
    "# work on test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Set Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transfer_set(dropped_classes):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = mlp.mnist1200().to(device)\n",
    "student = mlp.mnist400().to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock_ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
